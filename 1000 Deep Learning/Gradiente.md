2024-01-24, 11:51
### Contenido principal

El gradiente es un concepto fundamental en cálculo diferencial y vectorial. Representa la tasa de cambio máxima de una función en un punto específico y su dirección. En el contexto de funciones escalares, el gradiente se expresa como un vector que señala la dirección en la cual la función crece más rápidamente.

Dada una función escalar $f(x, y, z, \dots)$, el gradiente $\nabla f$ se define como el vector de derivadas parciales con respecto a cada una de las variables:
$$\nabla f = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}, \dots)$$

La interpretación geométrica del gradiente es: en un punto dado, el gradiente apunta en la dirección en la cual la función crece más rápidamente. La magnitud del gradiente indica la tasa de cambio en esa dirección. Si el gradiente es cero, estamos en un punto crítico donde la función alcanza un mínimo, máximo o punto de inflexión.

--- 
### Referencias

[[Neural Networks and Deep Learning#Descenso del gradiente.]]

---
### Anki
